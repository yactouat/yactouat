---
subject: 'Prompt engineering'
term: 'hallucinations'
---

An LLM hallucination occurs when the model generates false information.

This falsy generation occurs when the data fed to the model is misinterpreted. Since all generations are based on what has been seen before in the training data, sometimes this leads to the model generating weird outputs.

Moreover, generative AIs tend to output an innacurate response as a preferred way of dealing with uncertainty.
