---
subject: 'NLP'
term: 'tokens'
---

Tokens are the smallest meaningful unit of text. They can be words, punctuation, or numbers.

A token is approximately 4 characters or 0.75 English words long.

## OpenAI tokenizer

https://platform.openai.com/tokenizer